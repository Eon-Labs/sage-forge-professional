#!/usr/bin/env python3
"""
ðŸŽ¯ NT-NATIVE ADAPTIVE PARAMETER OPTIMIZATION 2025
=================================================

Bayesian optimization for auto-tuning strategy parameters in real-time.
Follows NautilusTrader patterns for bias-free operation and live trading compatibility.

Features:
- Gaussian Process optimization for parameter search
- Online parameter updates during trading
- Performance-based objective functions
- Convergence criteria and stability monitoring
- Integration with enhanced SOTA strategy
- Production-ready for live trading

Algorithms:
- Gaussian Process Regression (GPR) for parameter modeling
- Expected Improvement (EI) acquisition function
- Multi-objective optimization (profit vs risk)
- Online parameter adaptation
- Convergence detection

Author: Claude Code Assistant  
Date: 2025-07-19
License: MIT
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from collections import deque
import warnings
from datetime import datetime
import json
from pathlib import Path

# Rich console for enhanced output
try:
    from rich.console import Console
    console = Console()
except ImportError:
    class MockConsole:
        def print(self, *args, **kwargs):
            print(*args)
    console = MockConsole()


@dataclass
class ParameterBounds:
    """Parameter bounds for optimization."""
    name: str
    lower_bound: float
    upper_bound: float
    current_value: float
    parameter_type: str = "continuous"  # "continuous", "discrete", "categorical"


@dataclass
class OptimizationResult:
    """Result of parameter optimization."""
    parameters: Dict[str, float]
    objective_value: float
    acquisition_value: float
    iteration: int
    timestamp: datetime
    confidence_interval: Tuple[float, float]


class GaussianProcessOptimizer:
    """
    ðŸŽ¯ Gaussian Process Bayesian Optimization
    
    Optimizes strategy parameters using Gaussian Process regression
    with Expected Improvement acquisition function.
    """
    
    def __init__(self, parameter_bounds: List[ParameterBounds], 
                 acquisition_function: str = "expected_improvement",
                 exploration_weight: float = 0.01,
                 convergence_tolerance: float = 1e-4):
        self.parameter_bounds = {p.name: p for p in parameter_bounds}
        self.acquisition_function = acquisition_function
        self.exploration_weight = exploration_weight
        self.convergence_tolerance = convergence_tolerance
        
        # Optimization history
        self.parameter_history = []
        self.objective_history = []
        self.acquisition_history = []
        self.iteration_count = 0
        
        # GP parameters (simplified implementation)
        self.gp_mean = 0.0
        self.gp_variance = 1.0
        self.noise_variance = 1e-6
        self.length_scale = 1.0
        self.signal_variance = 1.0
        
        # Covariance matrix and its inverse (for GP predictions)
        self.K_inv = None
        self.alpha = None
        
        # Convergence tracking
        self.converged = False
        self.convergence_history = deque(maxlen=10)
        
        console.print(f"[green]ðŸŽ¯ Gaussian Process Optimizer initialized[/green]")
        console.print(f"[cyan]  â€¢ Parameters: {list(self.parameter_bounds.keys())}[/cyan]")
        console.print(f"[cyan]  â€¢ Acquisition: {acquisition_function}[/cyan]")
    
    def suggest_parameters(self) -> Dict[str, float]:
        """Suggest next parameter configuration to evaluate."""
        if len(self.parameter_history) < 2:
            # Random initialization for first few points
            return self._random_parameters()
        
        # Update GP model with current data
        self._update_gp_model()
        
        # Optimize acquisition function
        best_params = self._optimize_acquisition()
        
        return best_params
    
    def update_observation(self, parameters: Dict[str, float], objective_value: float):
        """Update optimizer with new observation."""
        self.parameter_history.append(parameters.copy())
        self.objective_history.append(objective_value)
        self.iteration_count += 1
        
        # Check convergence
        self._check_convergence()
        
        console.print(f"[dim cyan]ðŸŽ¯ Updated GP: iter {self.iteration_count}, "
                     f"obj={objective_value:.4f}, converged={self.converged}[/dim cyan]")
    
    def _random_parameters(self) -> Dict[str, float]:
        """Generate random parameters within bounds."""
        params = {}
        for name, bounds in self.parameter_bounds.items():
            if bounds.parameter_type == "continuous":
                value = np.random.uniform(bounds.lower_bound, bounds.upper_bound)
            elif bounds.parameter_type == "discrete":
                value = np.random.randint(bounds.lower_bound, bounds.upper_bound + 1)
            else:  # categorical
                value = bounds.current_value  # Keep current for categorical
            params[name] = value
        return params
    
    def _update_gp_model(self):
        """Update Gaussian Process model with current observations."""
        if len(self.parameter_history) < 2:
            return
        
        # Convert parameters to numpy array
        X = self._parameters_to_array(self.parameter_history)
        y = np.array(self.objective_history)
        
        # Compute covariance matrix (RBF kernel)
        K = self._rbf_kernel(X, X) + self.noise_variance * np.eye(len(X))
        
        try:
            # Compute inverse for predictions
            self.K_inv = np.linalg.inv(K)
            self.alpha = self.K_inv @ (y - self.gp_mean)
        except np.linalg.LinAlgError:
            # Add jitter for numerical stability
            K += 1e-6 * np.eye(len(X))
            self.K_inv = np.linalg.inv(K)
            self.alpha = self.K_inv @ (y - self.gp_mean)
    
    def _rbf_kernel(self, X1: np.ndarray, X2: np.ndarray) -> np.ndarray:
        """RBF (Gaussian) kernel for GP."""
        # Squared exponential kernel
        sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)
        return self.signal_variance * np.exp(-0.5 * sqdist / self.length_scale**2)
    
    def _gp_predict(self, X_new: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Predict mean and variance at new points."""
        if self.K_inv is None or len(self.parameter_history) < 2:
            # Prior prediction
            mean = np.full(X_new.shape[0], self.gp_mean)
            var = np.full(X_new.shape[0], self.signal_variance)
            return mean, var
        
        X_train = self._parameters_to_array(self.parameter_history)
        
        # Kernel between training and test points
        K_star = self._rbf_kernel(X_train, X_new)
        K_star_star = self._rbf_kernel(X_new, X_new)
        
        # Predictive mean and variance
        mean = self.gp_mean + K_star.T @ self.alpha
        var = np.diag(K_star_star) - np.diag(K_star.T @ self.K_inv @ K_star)
        
        # Ensure positive variance
        var = np.maximum(var, 1e-6)
        
        return mean, var
    
    def _optimize_acquisition(self) -> Dict[str, float]:
        """Optimize acquisition function to find next parameter configuration."""
        # Simple grid search for acquisition optimization
        n_candidates = 1000
        candidates = []
        
        # Generate random candidates
        for _ in range(n_candidates):
            candidate = self._random_parameters()
            candidates.append(candidate)
        
        # Convert to array for GP prediction
        X_candidates = self._parameters_to_array(candidates)
        
        # Predict with GP
        mean, var = self._gp_predict(X_candidates)
        std = np.sqrt(var)
        
        # Compute acquisition function
        if self.acquisition_function == "expected_improvement":
            acquisition_values = self._expected_improvement(mean, std)
        elif self.acquisition_function == "upper_confidence_bound":
            acquisition_values = self._upper_confidence_bound(mean, std)
        else:
            acquisition_values = mean  # Fallback to mean
        
        # Select best candidate
        best_idx = np.argmax(acquisition_values)
        best_params = candidates[best_idx]
        
        # Store acquisition value for logging
        self.acquisition_history.append(acquisition_values[best_idx])
        
        return best_params
    
    def _expected_improvement(self, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
        """Expected Improvement acquisition function."""
        if len(self.objective_history) == 0:
            return std
        
        best_observed = max(self.objective_history)
        
        # Expected improvement calculation
        improvement = mean - best_observed - self.exploration_weight
        Z = improvement / std
        
        # Handle numerical issues
        Z = np.clip(Z, -10, 10)
        
        # Compute EI using normal distribution
        from scipy.stats import norm
        ei = improvement * norm.cdf(Z) + std * norm.pdf(Z)
        
        return ei
    
    def _upper_confidence_bound(self, mean: np.ndarray, std: np.ndarray) -> np.ndarray:
        """Upper Confidence Bound acquisition function."""
        beta = 2.0  # Exploration parameter
        return mean + beta * std
    
    def _parameters_to_array(self, params_list: List[Dict[str, float]]) -> np.ndarray:
        """Convert parameter dictionaries to numpy array."""
        if not params_list:
            return np.array([]).reshape(0, len(self.parameter_bounds))
        
        # Normalize parameters to [0, 1] range
        normalized_params = []
        for params in params_list:
            normalized = []
            for name in sorted(self.parameter_bounds.keys()):
                bounds = self.parameter_bounds[name]
                value = params.get(name, bounds.current_value)
                # Normalize to [0, 1]
                norm_value = (value - bounds.lower_bound) / (bounds.upper_bound - bounds.lower_bound)
                normalized.append(norm_value)
            normalized_params.append(normalized)
        
        return np.array(normalized_params)
    
    def _check_convergence(self):
        """Check if optimization has converged."""
        if len(self.objective_history) < 5:
            return
        
        recent_objectives = self.objective_history[-5:]
        improvement = max(recent_objectives) - min(recent_objectives)
        
        self.convergence_history.append(improvement)
        
        if len(self.convergence_history) >= 5:
            avg_improvement = np.mean(list(self.convergence_history))
            if avg_improvement < self.convergence_tolerance:
                self.converged = True
    
    def get_best_parameters(self) -> Tuple[Dict[str, float], float]:
        """Get best parameters found so far."""
        if not self.objective_history:
            return {name: bounds.current_value for name, bounds in self.parameter_bounds.items()}, 0.0
        
        best_idx = np.argmax(self.objective_history)
        best_params = self.parameter_history[best_idx]
        best_objective = self.objective_history[best_idx]
        
        return best_params, best_objective
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """Get summary of optimization process."""
        if not self.objective_history:
            return {"status": "no_observations"}
        
        best_params, best_objective = self.get_best_parameters()
        
        return {
            "iterations": self.iteration_count,
            "converged": self.converged,
            "best_objective": best_objective,
            "best_parameters": best_params,
            "objective_history": list(self.objective_history[-10:]),  # Last 10
            "convergence_rate": np.mean(list(self.convergence_history)) if self.convergence_history else 0.0
        }


class AdaptiveParameterOptimizer:
    """
    ðŸŽ¯ Adaptive Parameter Optimizer for Trading Strategies
    
    Integrates Gaussian Process optimization with strategy performance evaluation.
    Automatically adjusts strategy parameters based on recent performance.
    """
    
    def __init__(self, strategy_parameters: Dict[str, Tuple[float, float]], 
                 optimization_frequency: int = 100,
                 performance_window: int = 50):
        self.optimization_frequency = optimization_frequency
        self.performance_window = performance_window
        
        # Create parameter bounds
        parameter_bounds = []
        for name, (lower, upper) in strategy_parameters.items():
            current_value = (lower + upper) / 2  # Start at midpoint
            bounds = ParameterBounds(name, lower, upper, current_value)
            parameter_bounds.append(bounds)
        
        # Initialize GP optimizer
        self.gp_optimizer = GaussianProcessOptimizer(parameter_bounds)
        
        # Performance tracking
        self.performance_history = deque(maxlen=1000)
        self.parameter_performance = {}
        self.current_parameters = {name: bounds.current_value 
                                 for name, bounds in self.gp_optimizer.parameter_bounds.items()}
        
        # Optimization state
        self.update_counter = 0
        self.last_optimization_counter = 0
        self.optimization_active = False
        
        console.print(f"[green]ðŸŽ¯ Adaptive Parameter Optimizer initialized[/green]")
        console.print(f"[cyan]  â€¢ Parameters: {list(self.current_parameters.keys())}[/cyan]")
        console.print(f"[cyan]  â€¢ Optimization frequency: every {optimization_frequency} updates[/cyan]")
    
    def update_performance(self, performance_metrics: Dict[str, float]):
        """Update with new performance observation."""
        self.update_counter += 1
        
        # Compute composite performance score
        performance_score = self._compute_performance_score(performance_metrics)
        self.performance_history.append({
            'score': performance_score,
            'metrics': performance_metrics.copy(),
            'parameters': self.current_parameters.copy(),
            'timestamp': datetime.now()
        })
        
        # Check if optimization should be triggered
        if (self.update_counter - self.last_optimization_counter) >= self.optimization_frequency:
            self._trigger_optimization()
    
    def _compute_performance_score(self, metrics: Dict[str, float]) -> float:
        """Compute composite performance score from multiple metrics."""
        # Default scoring function (can be customized)
        score = 0.0
        
        # Profit-related metrics (positive contribution)
        if 'recent_return' in metrics:
            score += metrics['recent_return'] * 10.0  # Weight returns highly
        
        if 'sharpe_ratio' in metrics:
            score += metrics['sharpe_ratio'] * 2.0
        
        if 'win_rate' in metrics:
            score += (metrics['win_rate'] - 0.5) * 5.0  # Bonus for >50% win rate
        
        # Risk-related metrics (negative contribution)
        if 'max_drawdown' in metrics:
            score -= abs(metrics['max_drawdown']) * 15.0  # Penalize drawdown heavily
        
        if 'volatility' in metrics:
            score -= metrics['volatility'] * 3.0  # Penalize excessive volatility
        
        # Consistency metrics
        if 'correlation' in metrics:
            score += abs(metrics['correlation']) * 1.0  # Reward predictability
        
        return score
    
    def _trigger_optimization(self):
        """Trigger parameter optimization based on recent performance."""
        if len(self.performance_history) < self.performance_window:
            return
        
        # Evaluate current parameter set
        recent_performance = list(self.performance_history)[-self.performance_window:]
        avg_performance = np.mean([p['score'] for p in recent_performance])
        
        # Update GP with current parameters and performance
        self.gp_optimizer.update_observation(self.current_parameters, avg_performance)
        
        # Suggest new parameters if not converged
        if not self.gp_optimizer.converged:
            new_parameters = self.gp_optimizer.suggest_parameters()
            self._update_parameters(new_parameters)
            
            console.print(f"[yellow]ðŸŽ¯ Parameter optimization triggered (iter {self.gp_optimizer.iteration_count})[/yellow]")
            console.print(f"[cyan]  â€¢ Performance score: {avg_performance:.4f}[/cyan]")
            console.print(f"[cyan]  â€¢ New parameters: {new_parameters}[/cyan]")
        else:
            console.print(f"[green]ðŸŽ¯ Parameter optimization converged[/green]")
        
        self.last_optimization_counter = self.update_counter
    
    def _update_parameters(self, new_parameters: Dict[str, float]):
        """Update current parameters with new values."""
        self.current_parameters.update(new_parameters)
        
        # Ensure parameters are within bounds
        for name, value in self.current_parameters.items():
            if name in self.gp_optimizer.parameter_bounds:
                bounds = self.gp_optimizer.parameter_bounds[name]
                self.current_parameters[name] = np.clip(
                    value, bounds.lower_bound, bounds.upper_bound
                )
    
    def get_current_parameters(self) -> Dict[str, float]:
        """Get current optimized parameters."""
        return self.current_parameters.copy()
    
    def get_optimization_status(self) -> Dict[str, Any]:
        """Get current optimization status."""
        gp_summary = self.gp_optimizer.get_optimization_summary()
        
        recent_performance = list(self.performance_history)[-10:]  # Last 10
        
        status = {
            "current_parameters": self.current_parameters.copy(),
            "optimization_active": not self.gp_optimizer.converged,
            "updates_since_optimization": self.update_counter - self.last_optimization_counter,
            "next_optimization_in": max(0, self.optimization_frequency - (self.update_counter - self.last_optimization_counter)),
            "recent_performance": [p['score'] for p in recent_performance],
            "gp_optimization": gp_summary
        }
        
        return status
    
    def save_optimization_history(self, filepath: str):
        """Save optimization history to file."""
        history = {
            "parameter_history": self.gp_optimizer.parameter_history,
            "objective_history": self.gp_optimizer.objective_history,
            "performance_history": [
                {
                    'score': p['score'],
                    'metrics': p['metrics'],
                    'parameters': p['parameters'],
                    'timestamp': p['timestamp'].isoformat()
                }
                for p in list(self.performance_history)
            ],
            "optimization_summary": self.get_optimization_status()
        }
        
        with open(filepath, 'w') as f:
            json.dump(history, f, indent=2, default=str)
        
        console.print(f"[cyan]ðŸ’¾ Optimization history saved to {filepath}[/cyan]")


def test_adaptive_optimization():
    """Test adaptive parameter optimization."""
    console.print("[yellow]ðŸ§ª Testing Adaptive Parameter Optimization...[/yellow]")
    
    # Define strategy parameters to optimize
    strategy_parameters = {
        "signal_threshold": (0.05, 0.3),
        "learning_rate": (0.001, 0.1),
        "feature_window": (20, 200),
        "risk_tolerance": (0.01, 0.1)
    }
    
    # Initialize optimizer
    optimizer = AdaptiveParameterOptimizer(
        strategy_parameters=strategy_parameters,
        optimization_frequency=20,  # Optimize every 20 updates
        performance_window=10
    )
    
    console.print("  Testing parameter optimization cycle...")
    
    # Simulate strategy performance with different parameters
    np.random.seed(42)
    
    for i in range(100):
        # Simulate performance metrics
        current_params = optimizer.get_current_parameters()
        
        # Simulate performance based on parameters (mock objective function)
        base_performance = 0.5
        
        # Better performance with certain parameter ranges
        if 0.1 <= current_params['signal_threshold'] <= 0.2:
            base_performance += 0.2
        if 0.01 <= current_params['learning_rate'] <= 0.05:
            base_performance += 0.15
        if 50 <= current_params['feature_window'] <= 100:
            base_performance += 0.1
        
        # Add noise
        performance = base_performance + np.random.normal(0, 0.1)
        
        # Create mock metrics
        metrics = {
            'recent_return': performance * 0.02,
            'sharpe_ratio': performance * 2.0,
            'max_drawdown': -performance * 0.05,
            'win_rate': 0.5 + performance * 0.2,
            'correlation': performance * 0.3
        }
        
        # Update optimizer
        optimizer.update_performance(metrics)
        
        # Log progress every 20 iterations
        if i % 20 == 0:
            status = optimizer.get_optimization_status()
            console.print(f"    Iteration {i}: GP iterations={status['gp_optimization'].get('iterations', 0)}, "
                         f"converged={status['optimization_active'] == False}")
    
    # Final results
    final_status = optimizer.get_optimization_status()
    best_params = final_status['current_parameters']
    gp_summary = final_status['gp_optimization']
    
    console.print(f"  Final Results:")
    console.print(f"    GP iterations: {gp_summary.get('iterations', 0)}")
    console.print(f"    Converged: {not final_status['optimization_active']}")
    console.print(f"    Best objective: {gp_summary.get('best_objective', 0):.4f}")
    console.print(f"    Best parameters:")
    for name, value in best_params.items():
        console.print(f"      {name}: {value:.4f}")
    
    # Test saving
    save_path = "test_optimization_history.json"
    optimizer.save_optimization_history(save_path)
    
    console.print("[green]âœ… Adaptive parameter optimization test completed![/green]")
    
    return optimizer


if __name__ == "__main__":
    console.print("[bold green]ðŸŽ¯ NT-Native Adaptive Parameter Optimization![/bold green]")
    console.print("[dim]Bayesian optimization for auto-tuning strategy parameters[/dim]")
    
    # Test the optimization system
    optimizer = test_adaptive_optimization()
    
    console.print("\n[green]ðŸŒŸ Ready for integration with enhanced SOTA strategy![/green]")