#!/usr/bin/env python3
"""
üîç TiRex Data Pipeline Vulnerability Analysis

Comprehensive deep-dive analysis of TiRex source code to identify and test 
data pipeline vulnerabilities and weaknesses not covered in basic security testing.

Focus: Data processing, tensor operations, scaling, context handling, and 
quantile generation vulnerabilities rather than adversarial attacks.
"""

import sys
import torch
import numpy as np
import logging
import traceback
import gc
import psutil
import os
from typing import Tuple, List, Dict, Any
from contextlib import contextmanager

# Configure test logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Memory monitoring
def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

@contextmanager
def memory_monitor(test_name: str, max_memory_mb: float = 8192):
    """Monitor memory usage during test execution"""
    initial_memory = get_memory_usage()
    try:
        yield
        final_memory = get_memory_usage()
        memory_delta = final_memory - initial_memory
        if memory_delta > max_memory_mb:
            raise MemoryError(f"Memory usage exceeded limit: {memory_delta:.1f}MB > {max_memory_mb}MB")
        logger.info(f"Memory delta for {test_name}: {memory_delta:.1f}MB")
    except Exception:
        gc.collect()  # Force garbage collection on error
        raise

def test_nan_handling_vulnerabilities():
    """Test 1: NaN handling data pipeline vulnerabilities"""
    print("\nüîç TEST 1: NaN Handling Data Pipeline Vulnerabilities")
    
    vulnerability_results = {}
    
    # Test 1a: Silent NaN-to-Zero conversion in model forward pass
    try:
        # Test the StandardScaler NaN handling directly
        sys.path.append('/home/tca/eon/nt/repos/tirex/src')
        from tirex.models.components import StandardScaler
        
        scaler = StandardScaler()
        
        # Test with all-NaN input (should not crash but behavior is undefined)
        all_nan_input = torch.full((2, 100), float('nan'))
        try:
            scaled_output, scale_state = scaler.scale(all_nan_input)
            loc, scale = scale_state
            
            # Check if scaling produces valid outputs
            if torch.isnan(scaled_output).all():
                vulnerability_results['all_nan_scaling'] = "‚ö†Ô∏è VULNERABILITY - All-NaN input produces all-NaN output (no error raised)"
            elif torch.isfinite(scaled_output).all():
                vulnerability_results['all_nan_scaling'] = "üîç BEHAVIOR - All-NaN input produces finite output (silent conversion)"
            else:
                vulnerability_results['all_nan_scaling'] = "üîç MIXED - All-NaN input produces mixed finite/nan output"
                
        except Exception as e:
            vulnerability_results['all_nan_scaling'] = f"‚úÖ SAFE - Exception raised: {type(e).__name__}"
        
        # Test 1b: NaN in scale parameters
        try:
            # Create input that would produce NaN scale (all same values)
            constant_input = torch.full((2, 100), 5.0)
            scaled_output, scale_state = scaler.scale(constant_input)
            loc, scale = scale_state
            
            if torch.isnan(scale).any() or torch.isinf(scale).any():
                vulnerability_results['nan_scale_parameters'] = "‚ö†Ô∏è VULNERABILITY - Scale parameters contain NaN/inf"
            elif scale.min() <= 0:
                vulnerability_results['nan_scale_parameters'] = "‚ö†Ô∏è VULNERABILITY - Scale parameters <= 0"
            else:
                vulnerability_results['nan_scale_parameters'] = "‚úÖ SAFE - Scale parameters are valid"
                
        except Exception as e:
            vulnerability_results['nan_scale_parameters'] = f"‚ö†Ô∏è VULNERABILITY - Scaling failed: {e}"
            
        # Test 1c: Rescaling with corrupted scale state
        try:
            # Test rescaling with NaN scale state
            corrupted_scale_state = (torch.tensor([0.0]), torch.tensor([float('nan')]))
            test_values = torch.randn(1, 10)
            
            rescaled = scaler.re_scale(test_values, corrupted_scale_state)
            if torch.isnan(rescaled).any():
                vulnerability_results['corrupted_rescaling'] = "‚ö†Ô∏è VULNERABILITY - Corrupted scale state produces NaN output"
            else:
                vulnerability_results['corrupted_rescaling'] = "üîç BEHAVIOR - Corrupted scale state produces finite output"
                
        except Exception as e:
            vulnerability_results['corrupted_rescaling'] = f"‚úÖ SAFE - Exception raised: {type(e).__name__}"
            
    except ImportError as e:
        vulnerability_results['nan_handling'] = f"‚ùå FAILED - Cannot import TiRex components: {e}"
    
    # Print results
    print("NaN Handling Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def test_context_length_vulnerabilities():
    """Test 2: Context length and memory handling vulnerabilities"""
    print("\nüîç TEST 2: Context Length and Memory Vulnerabilities")
    
    vulnerability_results = {}
    
    try:
        sys.path.append('/home/tca/eon/nt/repos/tirex/src')
        from tirex.models.components import PatchedUniTokenizer
        
        # Test 2a: Integer overflow in ceiling division
        try:
            tokenizer = PatchedUniTokenizer(patch_size=16)
            
            # Test with very large prediction_length that could cause integer overflow
            large_prediction_length = 2**31 - 1  # Max int32
            
            # Simulate the ceiling division logic from tirex.py:147
            try:
                remaining = -(large_prediction_length // -tokenizer.patch_size)
                if remaining > 0 and remaining < large_prediction_length:
                    vulnerability_results['integer_overflow'] = "‚úÖ SAFE - Large prediction_length handled correctly"
                else:
                    vulnerability_results['integer_overflow'] = "‚ö†Ô∏è VULNERABILITY - Integer overflow or invalid result"
                    
            except OverflowError:
                vulnerability_results['integer_overflow'] = "‚ö†Ô∏è VULNERABILITY - Overflow error with large prediction_length"
            except Exception as e:
                vulnerability_results['integer_overflow'] = f"‚ö†Ô∏è VULNERABILITY - Unexpected error: {e}"
                
        except Exception as e:
            vulnerability_results['integer_overflow'] = f"‚ùå FAILED - Cannot test: {e}"
            
        # Test 2b: Memory exhaustion with large contexts
        try:
            with memory_monitor("large_context_test", max_memory_mb=1024):  # 1GB limit
                tokenizer = PatchedUniTokenizer(patch_size=16)
                
                # Test with progressively larger contexts
                for size in [1000, 10000, 100000]:
                    try:
                        large_context = torch.randn(1, size) * 0.1
                        tokenized, state = tokenizer.context_input_transform(large_context)
                        
                        # Check if tokenization succeeded
                        if tokenized.shape[0] == 1:
                            continue  # This size is fine
                        else:
                            vulnerability_results['memory_exhaustion'] = f"‚ö†Ô∏è VULNERABILITY - Unexpected output shape at size {size}"
                            break
                            
                    except RuntimeError as e:
                        if "out of memory" in str(e).lower():
                            vulnerability_results['memory_exhaustion'] = f"‚ö†Ô∏è VULNERABILITY - OOM at size {size}"
                            break
                        else:
                            raise
                            
                if 'memory_exhaustion' not in vulnerability_results:
                    vulnerability_results['memory_exhaustion'] = "‚úÖ SAFE - Memory usage within bounds for tested sizes"
                    
        except MemoryError:
            vulnerability_results['memory_exhaustion'] = "‚ö†Ô∏è VULNERABILITY - Memory limit exceeded during test"
        except Exception as e:
            vulnerability_results['memory_exhaustion'] = f"‚ùå FAILED - {e}"
            
        # Test 2c: Context padding edge cases
        try:
            tokenizer = PatchedUniTokenizer(patch_size=16, patch_stride=4)
            
            # Test with context length that doesn't align with patch size/stride
            problematic_lengths = [1, 3, 5, 7, 15, 17, 31, 33]
            
            for length in problematic_lengths:
                context = torch.randn(1, length)
                try:
                    tokenized, _ = tokenizer.context_input_transform(context)
                    # Check if result contains reasonable values
                    if torch.isfinite(tokenized).float().mean() < 0.5:  # More than 50% non-finite
                        vulnerability_results['padding_edge_cases'] = f"‚ö†Ô∏è VULNERABILITY - Excessive non-finite values with length {length}"
                        break
                except Exception as e:
                    vulnerability_results['padding_edge_cases'] = f"‚ö†Ô∏è VULNERABILITY - Error with length {length}: {e}"
                    break
                    
            if 'padding_edge_cases' not in vulnerability_results:
                vulnerability_results['padding_edge_cases'] = "‚úÖ SAFE - All tested lengths handled correctly"
                
        except Exception as e:
            vulnerability_results['padding_edge_cases'] = f"‚ùå FAILED - {e}"
            
    except ImportError as e:
        vulnerability_results['context_length'] = f"‚ùå FAILED - Cannot import TiRex components: {e}"
    
    # Print results
    print("Context Length Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def test_tensor_operation_vulnerabilities():
    """Test 3: Tensor shape and operation vulnerabilities"""
    print("\nüîç TEST 3: Tensor Operation Vulnerabilities") 
    
    vulnerability_results = {}
    
    try:
        sys.path.append('/home/tca/eon/nt/repos/tirex/src')
        from tirex.api_adapter.standard_adapter import get_batches
        
        # Test 3a: Batch padding with mismatched tensor types/devices
        try:
            # Test with list of tensors of different dtypes
            mixed_dtype_tensors = [
                torch.randn(10).float(),
                torch.randn(15).double(),  # Different dtype
                torch.randn(8).float()
            ]
            
            try:
                batches = list(get_batches(mixed_dtype_tensors, batch_size=3))
                vulnerability_results['mixed_dtype_batching'] = "üîç BEHAVIOR - Mixed dtype tensors handled without error"
            except Exception as e:
                vulnerability_results['mixed_dtype_batching'] = f"‚úÖ SAFE - Mixed dtype tensors rejected: {type(e).__name__}"
                
        except Exception as e:
            vulnerability_results['mixed_dtype_batching'] = f"‚ùå FAILED - {e}"
            
        # Test 3b: Empty tensor handling in batching
        try:
            empty_tensors = [torch.empty(0), torch.randn(5), torch.empty(0)]
            
            try:
                batches = list(get_batches(empty_tensors, batch_size=2))
                vulnerability_results['empty_tensor_batching'] = "‚ö†Ô∏è VULNERABILITY - Empty tensors processed without validation"
            except Exception as e:
                vulnerability_results['empty_tensor_batching'] = f"‚úÖ SAFE - Empty tensors rejected: {type(e).__name__}"
                
        except Exception as e:
            vulnerability_results['empty_tensor_batching'] = f"‚ùå FAILED - {e}"
            
        # Test 3c: Extreme tensor dimensions
        try:
            # Test with very wide (many features) but short (few timesteps) tensor
            extreme_wide = torch.randn(1, 1)  # Minimal valid tensor
            extreme_long = torch.randn(1, 100000)  # Very long sequence
            
            for test_name, test_tensor in [("extreme_wide", extreme_wide), ("extreme_long", extreme_long)]:
                try:
                    batches = list(get_batches(test_tensor, batch_size=1))
                    batch_tensor, _ = next(iter(batches))
                    
                    if batch_tensor.shape == test_tensor.shape:
                        continue  # This dimension is handled correctly
                    else:
                        vulnerability_results[f'{test_name}_dimensions'] = f"‚ö†Ô∏è VULNERABILITY - Shape changed unexpectedly"
                        
                except Exception as e:
                    vulnerability_results[f'{test_name}_dimensions'] = f"‚ö†Ô∏è VULNERABILITY - Failed with {test_name}: {e}"
                    
            if not any('_dimensions' in k for k in vulnerability_results.keys()):
                vulnerability_results['extreme_dimensions'] = "‚úÖ SAFE - Extreme dimensions handled correctly"
                
        except Exception as e:
            vulnerability_results['extreme_dimensions'] = f"‚ùå FAILED - {e}"
            
        # Test 3d: Invalid batch sizes
        try:
            normal_tensor = torch.randn(2, 50)
            
            invalid_batch_sizes = [0, -1, -10]
            
            for batch_size in invalid_batch_sizes:
                try:
                    batches = list(get_batches(normal_tensor, batch_size=batch_size))
                    vulnerability_results['invalid_batch_sizes'] = f"‚ö†Ô∏è VULNERABILITY - Invalid batch_size {batch_size} accepted"
                    break
                except Exception as e:
                    continue  # This invalid batch size was correctly rejected
                    
            if 'invalid_batch_sizes' not in vulnerability_results:
                vulnerability_results['invalid_batch_sizes'] = "‚úÖ SAFE - Invalid batch sizes rejected"
                
        except Exception as e:
            vulnerability_results['invalid_batch_sizes'] = f"‚ùå FAILED - {e}"
            
    except ImportError as e:
        vulnerability_results['tensor_operations'] = f"‚ùå FAILED - Cannot import TiRex components: {e}"
    
    # Print results
    print("Tensor Operation Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def test_quantile_processing_vulnerabilities():
    """Test 4: Quantile processing and interpolation vulnerabilities"""
    print("\nüîç TEST 4: Quantile Processing Vulnerabilities")
    
    vulnerability_results = {}
    
    try:
        # Test 4a: Quantile interpolation with extreme values  
        try:
            # Simulate the quantile interpolation logic from predict_utils.py
            # Create predictions with extreme values that could break interpolation
            extreme_predictions = torch.tensor([
                [[-1e10, -1e5, -100, -1, 0, 1, 100, 1e5, 1e10]]  # [batch, timestep, quantiles]
            ])
            
            # Test quantile interpolation
            augmented = torch.cat([
                extreme_predictions[..., [0]], 
                extreme_predictions, 
                extreme_predictions[..., [-1]]
            ], dim=-1)
            
            requested_quantiles = [0.05, 0.25, 0.5, 0.75, 0.95]
            
            try:
                interpolated = torch.quantile(
                    augmented,
                    q=torch.tensor(requested_quantiles, dtype=augmented.dtype),
                    dim=-1
                ).permute(1, 2, 0)
                
                # Check if interpolation produced reasonable results
                if torch.isfinite(interpolated).all():
                    if torch.is_sorted(interpolated.flatten()):
                        vulnerability_results['extreme_quantile_interpolation'] = "‚úÖ SAFE - Extreme values handled correctly"
                    else:
                        vulnerability_results['extreme_quantile_interpolation'] = "‚ö†Ô∏è VULNERABILITY - Quantile ordering violated with extreme values"
                else:
                    vulnerability_results['extreme_quantile_interpolation'] = "‚ö†Ô∏è VULNERABILITY - Non-finite values in interpolated quantiles"
                    
            except Exception as e:
                vulnerability_results['extreme_quantile_interpolation'] = f"‚ö†Ô∏è VULNERABILITY - Quantile interpolation failed: {e}"
                
        except Exception as e:
            vulnerability_results['extreme_quantile_interpolation'] = f"‚ùå FAILED - {e}"
            
        # Test 4b: Out-of-range quantile requests
        try:
            # Simulate requesting quantiles outside the training range
            training_quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
            requested_quantiles = [0.01, 0.05, 0.5, 0.95, 0.99]  # Outside range
            
            normal_predictions = torch.randn(1, 10, 9)  # [batch, timesteps, quantiles]
            
            # Check range validation logic
            requested_set = set(requested_quantiles)
            training_set = set(training_quantiles)
            
            if requested_set.issubset(training_set):
                vulnerability_results['out_of_range_quantiles'] = "‚úÖ SAFE - Only in-range quantiles requested"
            else:
                min_req, max_req = min(requested_quantiles), max(requested_quantiles)
                min_train, max_train = min(training_quantiles), max(training_quantiles)
                
                if min_req < min_train or max_req > max_train:
                    # This should trigger clamping behavior - test if it's handled safely
                    try:
                        # Simulate clamping logic
                        clamped_predictions = torch.clamp(normal_predictions, min=-1e6, max=1e6)
                        vulnerability_results['out_of_range_quantiles'] = "üîç BEHAVIOR - Out-of-range quantiles trigger clamping"
                    except Exception as e:
                        vulnerability_results['out_of_range_quantiles'] = f"‚ö†Ô∏è VULNERABILITY - Clamping logic failed: {e}"
                        
        except Exception as e:
            vulnerability_results['out_of_range_quantiles'] = f"‚ùå FAILED - {e}"
            
        # Test 4c: Quantile ordering validation
        try:
            # Create predictions with reversed quantile ordering (should be monotonic)
            reversed_quantiles = torch.tensor([
                [[9, 8, 7, 6, 5, 4, 3, 2, 1]]  # Reversed ordering
            ]).float()
            
            # Check if quantile ordering is validated
            diffs = reversed_quantiles[..., 1:] - reversed_quantiles[..., :-1]
            if torch.any(diffs < 0):
                vulnerability_results['quantile_ordering_validation'] = "‚ö†Ô∏è VULNERABILITY - Reversed quantile ordering not detected"
            else:
                vulnerability_results['quantile_ordering_validation'] = "‚úÖ SAFE - Quantile ordering maintained"
                
        except Exception as e:
            vulnerability_results['quantile_ordering_validation'] = f"‚ùå FAILED - {e}"
            
        # Test 4d: Median selection edge cases
        try:
            # Test median extraction with even number of quantiles
            even_quantiles = torch.randn(1, 5, 8)  # 8 quantiles instead of 9
            
            try:
                # Simulate median extraction logic: training_quantile_levels.index(0.5)
                # This would fail if 0.5 is not in the training quantiles
                standard_quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
                if 0.5 in standard_quantiles:
                    median_idx = standard_quantiles.index(0.5)
                    if median_idx < even_quantiles.shape[-1]:
                        median = even_quantiles[:, :, median_idx]
                        vulnerability_results['median_extraction'] = "‚úÖ SAFE - Median extraction handled correctly"
                    else:
                        vulnerability_results['median_extraction'] = "‚ö†Ô∏è VULNERABILITY - Median index out of bounds"
                else:
                    vulnerability_results['median_extraction'] = "‚ö†Ô∏è VULNERABILITY - No median quantile in training set"
                    
            except Exception as e:
                vulnerability_results['median_extraction'] = f"‚ö†Ô∏è VULNERABILITY - Median extraction failed: {e}"
                
        except Exception as e:
            vulnerability_results['median_extraction'] = f"‚ùå FAILED - {e}"
            
    except Exception as e:
        vulnerability_results['quantile_processing'] = f"‚ùå FAILED - General error: {e}"
    
    # Print results
    print("Quantile Processing Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def test_device_precision_vulnerabilities():
    """Test 5: Device handling and precision vulnerabilities"""
    print("\nüîç TEST 5: Device and Precision Vulnerabilities")
    
    vulnerability_results = {}
    
    # Test 5a: Device mismatch handling
    try:
        # Test tensor operations with mixed devices (if multiple devices available)
        if torch.cuda.is_available():
            cpu_tensor = torch.randn(5, 10)
            cuda_tensor = torch.randn(5, 10).cuda()
            
            try:
                # This should fail - test if error is handled gracefully
                mixed_result = cpu_tensor + cuda_tensor
                vulnerability_results['device_mismatch'] = "‚ö†Ô∏è VULNERABILITY - Mixed device operations succeeded unexpectedly"
            except RuntimeError as e:
                if "Expected all tensors to be on the same device" in str(e):
                    vulnerability_results['device_mismatch'] = "‚úÖ SAFE - Device mismatch properly detected"
                else:
                    vulnerability_results['device_mismatch'] = f"‚ö†Ô∏è VULNERABILITY - Unexpected device error: {e}"
        else:
            vulnerability_results['device_mismatch'] = "‚ÑπÔ∏è SKIP - CUDA not available for device mismatch test"
            
    except Exception as e:
        vulnerability_results['device_mismatch'] = f"‚ùå FAILED - {e}"
        
    # Test 5b: Precision loss in dtype conversions
    try:
        # Test precision loss with extreme values
        high_precision = torch.tensor([1.123456789123456789], dtype=torch.double)
        
        # Convert to lower precision
        converted = high_precision.to(torch.float32).to(torch.float16).to(torch.float32)
        
        precision_loss = torch.abs(high_precision.float() - converted).item()
        
        if precision_loss > 1e-6:
            vulnerability_results['precision_loss'] = f"‚ö†Ô∏è VULNERABILITY - Significant precision loss: {precision_loss:.2e}"
        else:
            vulnerability_results['precision_loss'] = "‚úÖ SAFE - Minimal precision loss in conversions"
            
    except Exception as e:
        vulnerability_results['precision_loss'] = f"‚ùå FAILED - {e}"
        
    # Test 5c: Automatic device selection edge cases
    try:
        # Test device string parsing edge cases
        invalid_device_strings = ["cuda:-1", "cuda:999", "invalid_device", "", "cpu:0"]
        
        for device_str in invalid_device_strings:
            try:
                device = torch.device(device_str)
                test_tensor = torch.randn(5).to(device)
                # If we get here without error, check if it's an expected case
                if device_str in ["cpu:0"]:  # This might be valid
                    continue
                else:
                    vulnerability_results['invalid_device_strings'] = f"‚ö†Ô∏è VULNERABILITY - Invalid device string accepted: {device_str}"
                    break
            except Exception:
                continue  # Invalid device string correctly rejected
                
        if 'invalid_device_strings' not in vulnerability_results:
            vulnerability_results['invalid_device_strings'] = "‚úÖ SAFE - Invalid device strings properly rejected"
            
    except Exception as e:
        vulnerability_results['invalid_device_strings'] = f"‚ùå FAILED - {e}"
        
    # Test 5d: Memory leak in device transfers
    try:
        with memory_monitor("device_transfer_test", max_memory_mb=512):
            initial_memory = get_memory_usage()
            
            # Perform many device transfers
            for i in range(100):
                tensor = torch.randn(100, 100)
                if torch.cuda.is_available():
                    tensor = tensor.cuda().cpu()
                else:
                    tensor = tensor.clone()  # Simulate transfer
                del tensor
                
            # Force garbage collection
            gc.collect()
            final_memory = get_memory_usage()
            
            memory_leak = final_memory - initial_memory
            if memory_leak > 50:  # 50MB threshold
                vulnerability_results['device_transfer_memory_leak'] = f"‚ö†Ô∏è VULNERABILITY - Possible memory leak: {memory_leak:.1f}MB"
            else:
                vulnerability_results['device_transfer_memory_leak'] = "‚úÖ SAFE - No significant memory leak detected"
                
    except MemoryError:
        vulnerability_results['device_transfer_memory_leak'] = "‚ö†Ô∏è VULNERABILITY - Memory limit exceeded during transfers"
    except Exception as e:
        vulnerability_results['device_transfer_memory_leak'] = f"‚ùå FAILED - {e}"
    
    # Print results
    print("Device and Precision Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def test_model_loading_vulnerabilities():
    """Test 6: Model loading and initialization vulnerabilities"""
    print("\nüîç TEST 6: Model Loading and Initialization Vulnerabilities")
    
    vulnerability_results = {}
    
    try:
        sys.path.append('/home/tca/eon/nt/repos/tirex/src')
        from tirex.base import parse_hf_repo_id, load_model
        
        # Test 6a: Path parsing vulnerabilities
        try:
            malformed_paths = [
                "",  # Empty string
                "/",  # Just slash
                "single_part",  # No slash
                "too/many/parts/here",  # Too many parts
                "../../etc/passwd",  # Path traversal attempt
                "user/../admin/model",  # Path traversal in model path
                "user/model/../../secrets",  # Complex path traversal
            ]
            
            for path in malformed_paths:
                try:
                    parsed = parse_hf_repo_id(path)
                    if "/" in parsed and len(parsed.split("/")) == 2:
                        continue  # This might be valid
                    else:
                        vulnerability_results['path_parsing'] = f"‚ö†Ô∏è VULNERABILITY - Malformed path accepted: {path} -> {parsed}"
                        break
                except Exception:
                    continue  # Malformed path correctly rejected
                    
            if 'path_parsing' not in vulnerability_results:
                vulnerability_results['path_parsing'] = "‚úÖ SAFE - Malformed paths properly rejected"
                
        except Exception as e:
            vulnerability_results['path_parsing'] = f"‚ùå FAILED - {e}"
            
        # Test 6b: Model registry manipulation
        try:
            # Test if model registry can be corrupted
            from tirex.base import PretrainedModel
            
            original_registry = PretrainedModel.REGISTRY.copy()
            
            try:
                # Try to manipulate registry
                PretrainedModel.REGISTRY["test_model"] = "not_a_class"
                
                # Try to load the corrupted model
                try:
                    model = load_model("test_user/test_model")
                    vulnerability_results['registry_manipulation'] = "‚ö†Ô∏è VULNERABILITY - Corrupted registry entry accepted"
                except Exception:
                    vulnerability_results['registry_manipulation'] = "‚úÖ SAFE - Corrupted registry entry rejected"
                    
            finally:
                # Restore original registry
                PretrainedModel.REGISTRY = original_registry
                
        except Exception as e:
            vulnerability_results['registry_manipulation'] = f"‚ùå FAILED - {e}"
            
        # Test 6c: Invalid model configuration handling
        try:
            # Test with invalid model configuration
            from tirex.models.tirex import TiRexZero, TiRexZeroConfig
            
            invalid_configs = [
                {"input_patch_size": -1, "output_patch_size": 16, "quantiles": [0.5], "block_kwargs": {}, "input_ff_dim": 64},
                {"input_patch_size": 16, "output_patch_size": -1, "quantiles": [0.5], "block_kwargs": {}, "input_ff_dim": 64},
                {"input_patch_size": 16, "output_patch_size": 8, "quantiles": [0.5], "block_kwargs": {}, "input_ff_dim": 64},  # Mismatch
                {"input_patch_size": 16, "output_patch_size": 16, "quantiles": [], "block_kwargs": {}, "input_ff_dim": 64},  # Empty quantiles
                {"input_patch_size": 16, "output_patch_size": 16, "quantiles": [0.5, 0.3, 0.7], "block_kwargs": {}, "input_ff_dim": -1},  # Negative dim
            ]
            
            for i, config in enumerate(invalid_configs):
                try:
                    # This should fail during initialization
                    model = TiRexZero(config)
                    vulnerability_results['invalid_config'] = f"‚ö†Ô∏è VULNERABILITY - Invalid config {i} accepted"
                    break
                except Exception:
                    continue  # Invalid config correctly rejected
                    
            if 'invalid_config' not in vulnerability_results:
                vulnerability_results['invalid_config'] = "‚úÖ SAFE - Invalid configurations properly rejected"
                
        except Exception as e:
            vulnerability_results['invalid_config'] = f"‚ùå FAILED - {e}"
            
    except ImportError as e:
        vulnerability_results['model_loading'] = f"‚ùå FAILED - Cannot import TiRex components: {e}"
    
    # Print results
    print("Model Loading Vulnerability Results:")
    for vuln, result in vulnerability_results.items():
        print(f"  {vuln}: {result}")
    
    return len([r for r in vulnerability_results.values() if "‚úÖ SAFE" in r]) / max(len(vulnerability_results), 1)


def run_comprehensive_vulnerability_analysis():
    """Run complete TiRex data pipeline vulnerability analysis"""
    print("üîç TIREX DATA PIPELINE VULNERABILITY ANALYSIS")
    print("=" * 60)
    print("Focus: Data processing, tensor operations, scaling, and quantile generation")
    print("Scope: Non-adversarial vulnerabilities and edge cases in data pipeline")
    print("=" * 60)
    
    test_functions = [
        ("NaN Handling Vulnerabilities", test_nan_handling_vulnerabilities),
        ("Context Length Vulnerabilities", test_context_length_vulnerabilities), 
        ("Tensor Operation Vulnerabilities", test_tensor_operation_vulnerabilities),
        ("Quantile Processing Vulnerabilities", test_quantile_processing_vulnerabilities),
        ("Device & Precision Vulnerabilities", test_device_precision_vulnerabilities),
        ("Model Loading Vulnerabilities", test_model_loading_vulnerabilities),
    ]
    
    results = {}
    vulnerability_count = 0
    total_tests = 0
    
    for test_name, test_func in test_functions:
        try:
            print(f"\n{'='*30}")
            safety_ratio = test_func()
            results[test_name] = safety_ratio
            
            # Count vulnerabilities (lower safety ratio = more vulnerabilities)
            if safety_ratio < 0.8:
                status = f"‚ö†Ô∏è VULNERABILITIES FOUND (Safety: {safety_ratio:.1%})"
                vulnerability_count += 1
            else:
                status = f"‚úÖ MOSTLY SAFE (Safety: {safety_ratio:.1%})"
                
            print(f"{test_name}: {status}")
            total_tests += 1
            
        except Exception as e:
            print(f"‚ùå {test_name} CRASHED: {e}")
            print(f"Traceback: {traceback.format_exc()}")
            results[test_name] = 0.0
            vulnerability_count += 1
            total_tests += 1
    
    # Final analysis
    print(f"\n{'='*60}")
    print("üîç COMPREHENSIVE VULNERABILITY ANALYSIS RESULTS")  
    print(f"{'='*60}")
    
    overall_safety = sum(results.values()) / max(len(results), 1)
    
    for test_name, safety_ratio in results.items():
        if safety_ratio < 0.5:
            status = "üö® HIGH RISK"
        elif safety_ratio < 0.8:
            status = "‚ö†Ô∏è MEDIUM RISK"  
        else:
            status = "‚úÖ LOW RISK"
        print(f"{test_name}: {status} (Safety: {safety_ratio:.1%})")
    
    print(f"\nüìä OVERALL SAFETY ASSESSMENT: {overall_safety:.1%}")
    print(f"üìà VULNERABILITY CATEGORIES FOUND: {vulnerability_count}/{total_tests}")
    
    if overall_safety >= 0.8:
        print("üõ°Ô∏è TiRex data pipeline shows GOOD resilience to edge cases")
        assessment = "GOOD"
    elif overall_safety >= 0.6:
        print("‚ö†Ô∏è TiRex data pipeline shows MODERATE vulnerabilities requiring attention")
        assessment = "MODERATE"
    else:
        print("üö® TiRex data pipeline shows SIGNIFICANT vulnerabilities requiring immediate attention")
        assessment = "CRITICAL"
    
    print(f"\nüîç GUARDIAN SYSTEM IMPACT:")
    print(f"Guardian system protects against {vulnerability_count} vulnerability categories")
    print(f"Additional Guardian protections recommended for data pipeline edge cases")
    
    return assessment, vulnerability_count, overall_safety


if __name__ == "__main__":
    print("Starting TiRex Data Pipeline Vulnerability Analysis...")
    assessment, vuln_count, safety_score = run_comprehensive_vulnerability_analysis()
    
    # Exit code based on assessment
    if assessment == "GOOD":
        sys.exit(0)
    elif assessment == "MODERATE":
        sys.exit(1)
    else:  # CRITICAL
        sys.exit(2)