### TOKENIZED Layer ‚Äî TiRex Univariate Input Architecture

**‚ö†Ô∏è CRITICAL CLARIFICATION**: TiRex is a **univariate forecasting model** - processes single time series only

**üìã EMPIRICAL PROOF**: [Validation Results](../../implementation/tirex/empirical-validation/TIREX_EMPIRICAL_FINDINGS_COMPREHENSIVE.md) | [Source Code Evidence](../../../tests/validation/definitive_signal_proof_test.py)

**üõ°Ô∏è MANDATORY GUARDIAN PROTECTION**: All TiRex processing requires Guardian system due to 6 critical vulnerability categories. Direct calls PROHIBITED.

**Native TiRex Component**: `PatchedUniTokenizer.context_input_transform()`  
**Processing Flow**: `CONTEXT[single_series]` ‚Üí `StandardScaler.scale()` ‚Üí `_Patcher()` ‚Üí `patched_tensor[batch, num_patches, patch_size]`  
**Architecture Reality**: **UNIVARIATE ONLY** - cannot process multiple features simultaneously

**üî¨ SMOKING GUN**: TiRex source code Line 106: `assert data.ndim == 2` - hardcoded 2D requirement `[batch_size, sequence_length]` only

---

#### Executive Summary

**TiRex Architecture Reality**: Designed as univariate time series forecasting model

- **Input Requirement**: Single time series only (e.g., close prices)
- **Processing**: Patches univariate time series into `[batch, num_patches, patch_size]` format
- **Architecture Alignment**: Properly designed for single-variable forecasting
- **üìä EMPIRICAL VALIDATION**: All multi-dimensional inputs (`[1, 128, 5]`, `[128, 5, 1]`, etc.) fail with AssertionError

**Optimization Strategy**: Focus on **input quality** and **temporal features** within univariate constraint.

**üß™ VALIDATION STATUS**: ‚úÖ **EMPIRICALLY CONFIRMED** through comprehensive testing - [View Test Results](../../implementation/tirex/empirical-validation/TIREX_EMPIRICAL_FINDINGS_COMPREHENSIVE.md)

---

#### TiRex TOKENIZED Architecture ‚Äî Single Univariate Input

**üî¨ EMPIRICAL REALITY**: TiRex processes exactly ONE univariate time series with configurable context length

**Core TiRex Input Parameters**:

- **Input Series**: Single time series only (e.g., close prices)
- **Context Length**: Number of historical timesteps to process (e.g., 288 = 6 hours of 5-min data)
- **Shape Requirement**: `[batch_size, context_length]` where `batch_size=1` for single series

**Example TiRex Input**:

```python
# Select ONE univariate series from available context data
close_series = context_data['close'][-288:]  # Last 288 timesteps (6 hours)
context_tensor = torch.tensor(close_series, dtype=torch.float32).unsqueeze(0)  # [1, 288]

# This is the ONLY input TiRex can process - one series, configurable length
```

**Architecture Constraints**:

- **Univariate Only**: TiRex architecturally limited to single time series input
- **No Multi-Feature Support**: Cannot process OHLCV, volume, or indicators simultaneously
- **Single Variable Forecasting**: Outputs quantiles for same input variable only
- **Patch Processing**: Applies to temporal dimension, not feature dimension

**Optimization Focus**: Input quality and temporal preprocessing within univariate constraint.

---

#### TOKENIZED Layer Optimization ‚Äî Univariate Input Selection & Context Length

**Two Optimization Dimensions**:

1. **Series Selection**: Choose optimal univariate time series from available context data
2. **Context Length**: Configure historical window length for TiRex processing

##### Univariate Series Selection Options

**‚ö†Ô∏è CRITICAL**: Select exactly **ONE** time series - TiRex cannot process multiple simultaneously

**üî¨ EMPIRICALLY VALIDATED**: Based on comprehensive TiRex repository analysis, optimal preprocessing uses **minimal transformation**

| Input Series             | Source Data Required    | TiRex Native Processing                     | Empirical Evidence                   | Recommended Priority |
| ------------------------ | ----------------------- | ------------------------------------------- | ------------------------------------ | -------------------- |
| **raw_close**            | `context_data['close']` | Per-series StandardScaler only              | ‚úÖ **OPTIMAL** (repository evidence) | **PRIMARY**          |
| **typical_price**        | OHLC data required      | `(high + low + close) / 3` ‚Üí StandardScaler | Cross-domain success                 | **SECONDARY**        |
| **volume_weighted**      | Price + volume required | VWAP ‚Üí StandardScaler                       | Market structure aware               | **TERTIARY**         |
| ~~**log_returns**~~      | ‚ùå Not recommended      | ‚ùå No repository evidence                   | ‚ùå Over-processing                   | **AVOID**            |
| ~~**normalized_close**~~ | ‚ùå Redundant            | ‚ùå TiRex already standardizes               | ‚ùå Double processing                 | **AVOID**            |

##### Dynamic Context Length Configuration

**üöÄ BREAKTHROUGH DISCOVERY**: TiRex supports **runtime dynamic context length adjustment** via `max_context` parameter!

**Two-Tier Context System** (from repository analysis):

- **`train_ctx_len`**: Model's training context length (fixed at initialization)
- **`max_context`**: Runtime-configurable parameter for inference-time adjustment

**üèÜ EMPIRICALLY VALIDATED Context Length Guidelines** (RTX 4090 Performance Testing):

| Context Length | Time Window (5min bars) | Memory Usage | Inference Speed | Forecast Quality | Production Status | Empirical Evidence |
| -------------- | ----------------------- | ------------ | --------------- | ---------------- | ----------------- | ------------------- |
| 144            | ~12 hours               | 146MB        | **SLOW** (19.2ms) | Good             | ‚ö†Ô∏è **AVOID** - Paradox | ‚úÖ **EMPIRICALLY CONFIRMED SLOW** |
| 288            | ~24 hours               | 284MB        | **FAST** (9.5ms) | Better           | ‚úÖ **OPTIMAL SPEED**    | ‚úÖ Comprehensive testing |
| 384            | ~32 hours               | 312MB        | **FASTEST** (9.4ms) | Better         | ‚úÖ **OPTIMAL BALANCE**  | ‚úÖ Sweet spot validation |
| 512            | ~42 hours               | 338MB        | **FAST** (9.7ms) | **BEST**        | ‚úÖ **QUALITY FOCUSED** | ‚úÖ Long-term forecasting |
| 1024           | ~85 hours               | 353MB        | **FAST** (9.7ms) | Research        | ‚úÖ Research contexts   | ‚úÖ Large context validation |
| 2048           | ~7.1 days               | 284MB        | **FAST** (9.3ms) | Research        | ‚úÖ Multi-day analysis  | ‚úÖ Extended validation |
| 4096           | ~14.2 days              | 303MB        | **FAST** (9.5ms) | Research        | ‚úÖ Long-term research  | ‚úÖ Extended validation |
| 16384          | ~56.9 days              | 311MB        | **GOOD** (10.1ms) | Maximum        | ‚úÖ **EXTREME RESEARCH** | ‚úÖ Large context validation |

**üö® CRITICAL DISCOVERY - 144 Timestep Paradox**: 144 timesteps are **SLOWER** than larger contexts (19.2ms vs 9.4-9.7ms) - empirically confirmed across multiple benchmark suites.

**üìä MEMORY SCALING BREAKTHROUGH**: Memory usage is virtually **FLAT** beyond 2048 timesteps - contexts up to 56.9 days use only ~310MB total.

**Dynamic Configuration Benefits**:

- **Zero-shot adaptation**: No model retraining required for different context lengths
- **Runtime optimization**: Adjust context length based on available data/computational resources
- **Variable batch processing**: Different context lengths within same batch (auto-padded)

**üèÜ PRODUCTION-VALIDATED CONTEXT LENGTH OPTIMIZATION STRATEGIES** (RTX 4090 Empirical Testing):

**For Production Trading Systems**:

1. **Speed-Optimized Strategy**: Use **384 timesteps** (9.4ms - empirically fastest, 1.3-day context)
2. **Balanced Production**: Use **288 timesteps** (9.5ms - proven reliable, 1-day context)  
3. **Quality-Focused**: Use **512 timesteps** (9.7ms - minimal speed penalty, 1.8-day context)
4. **‚ö†Ô∏è AVOID 144 Timesteps**: Paradoxically slow (19.2ms) - confirmed across multiple test suites
5. **Research/Academic**: Use **2048-16384 timesteps** for multi-day pattern analysis (minimal speed penalty)

**For Backtesting Time Optimization** (Empirically Validated):

- **100K predictions with 384 timesteps**: ~15.7 minutes (production optimal)
- **100K predictions with 512 timesteps**: ~16.2 minutes (quality focused)
- **100K predictions with 2048 timesteps**: ~15.5 minutes (research grade - surprisingly fast)

**üöÄ PRODUCTION-GRADE CONTEXT OPTIMIZATION FRAMEWORK** (Empirically Validated):

```python
def calculate_optimal_context_empirical(use_case='production', gpu_memory_gb=24):
    """Empirically-validated context length selection based on comprehensive RTX 4090 testing"""
    
    # RTX 4090 empirically validated configurations
    configurations = {
        'speed_optimal': {
            'context_length': 384,
            'inference_ms': 9.4,
            'memory_mb': 312,
            'use_case': 'High-frequency trading, rapid iteration'
        },
        'production': {
            'context_length': 288, 
            'inference_ms': 9.5,
            'memory_mb': 284,
            'use_case': 'Standard production backtesting'
        },
        'quality': {
            'context_length': 512,
            'inference_ms': 9.7, 
            'memory_mb': 338,
            'use_case': 'Quality-focused forecasting'
        },
        'research': {
            'context_length': 2048,
            'inference_ms': 9.3,
            'memory_mb': 284,
            'use_case': 'Multi-day pattern analysis'
        },
        'academic': {
            'context_length': 16384,
            'inference_ms': 10.1,
            'memory_mb': 311,
            'use_case': 'Long-range dependency research'
        }
    }
    
    # Memory feasibility check (RTX 4090 baseline)
    if gpu_memory_gb < 8:
        # GPU with <8GB - stick to smaller contexts
        return configurations['production']  # 288 timesteps, 284MB
    elif gpu_memory_gb >= 24:
        # RTX 4090 class GPU - all contexts feasible
        return configurations[use_case]
    else:
        # Mid-range GPU - avoid extreme contexts
        safe_configs = ['speed_optimal', 'production', 'quality', 'research']
        return configurations[use_case if use_case in safe_configs else 'production']

# Empirical usage examples
speed_config = calculate_optimal_context_empirical(use_case='speed_optimal')  # 384 timesteps, 9.4ms
production_config = calculate_optimal_context_empirical(use_case='production')  # 288 timesteps, 9.5ms  
quality_config = calculate_optimal_context_empirical(use_case='quality')      # 512 timesteps, 9.7ms
research_config = calculate_optimal_context_empirical(use_case='research')    # 2048 timesteps, 9.3ms
```

**‚ö†Ô∏è CRITICAL DISCOVERY**: Traditional "seasonal coverage" formulas are **empirically invalidated** - our testing shows:
- Longer contexts don't linearly improve forecasting (plateau effect after 512)  
- Memory scaling is **FLAT** (310MB for all contexts >2K)
- **144 timestep paradox**: Shorter isn't faster (19.2ms vs 9.4ms for 384)

---

#### Input Quality Comparison

| Aspect                   | Basic Input          | Optimized Input              | Benefit            |
| ------------------------ | -------------------- | ---------------------------- | ------------------ |
| **Input Series**         | Raw close prices     | Engineered univariate series | Better signal      |
| **Preprocessing**        | Basic scaling        | Stationary transformation    | Model-friendly     |
| **Temporal Features**    | Static normalization | Regime-aware preprocessing   | Context awareness  |
| **Data Quality**         | Raw exchange data    | Cleaned, validated series    | Robust forecasting |
| **Expected Performance** | Baseline             | **10-30% improvement**       | Input optimization |

---

#### TiRex Native Component Integration ‚Äî Univariate Reality

##### PatchedUniTokenizer Correct Usage

```python
# CORRECT - TiRex univariate processing
tokenizer = PatchedUniTokenizer(patch_size=12, scaler=StandardScaler())

# Option 1: Raw close prices
close_series = torch.tensor(close_prices)  # Shape: [sequence_length]
tokenized_tensor, scaler_state = tokenizer.context_input_transform(close_series)

# Option 2: Log returns (often better for forecasting)
returns = torch.tensor(np.log(close_prices[1:] / close_prices[:-1]))
tokenized_tensor, scaler_state = tokenizer.context_input_transform(returns)

# IMPOSSIBLE - TiRex cannot do this:
# multi_features = torch.stack([close, volume, rsi], dim=-1)  # ‚ùå NOT SUPPORTED
```

##### StandardScaler State Management

```python
# Correct scaler_state for single time series
scaler_state = {
    'loc': mean_value,    # Scaling mean for univariate series
    'scale': std_value    # Scaling std for univariate series
}
# Shape: Single values, not arrays (because univariate)
```

##### Complete TiRex Input Configuration

```python
# TiRex univariate input configuration
def prepare_tirex_input(context_data, series_choice='close', context_length=288):
    """Prepare single univariate input for TiRex with configurable context length"""

    # Step 1: Select ONE univariate series (empirically-backed options only)
    if series_choice == 'close':
        series = context_data['close']  # ‚úÖ OPTIMAL: Repository evidence
    elif series_choice == 'typical_price':
        series = (context_data['high'] + context_data['low'] + context_data['close']) / 3  # ‚úÖ VALID: Raw price variant
    elif series_choice == 'volume_weighted':
        # Calculate VWAP then let TiRex standardize
        series = (context_data['close'] * context_data['volume']).rolling(20).sum() / context_data['volume'].rolling(20).sum()
    else:
        raise ValueError(f"Series choice '{series_choice}' not empirically validated. Use: 'close', 'typical_price', 'volume_weighted'")
    # NOTE: No log returns or manual normalization - TiRex StandardScaler handles all preprocessing

    # Step 2: Extract context window (most recent N timesteps)
    if len(series) < context_length:
        raise ValueError(f"Insufficient data: need {context_length}, have {len(series)}")

    context_window = series[-context_length:]  # Last N timesteps

    # Step 3: Convert to TiRex-compatible tensor shape [1, context_length]
    context_tensor = torch.tensor(context_window.values, dtype=torch.float32).unsqueeze(0)

    return context_tensor

# STATIC CONTEXT LENGTH EXAMPLES
context_short = prepare_tirex_input(market_data, series_choice='close', context_length=144)    # [1, 144]
context_medium = prepare_tirex_input(market_data, series_choice='close', context_length=288)   # [1, 288]
context_long = prepare_tirex_input(market_data, series_choice='close', context_length=512)    # [1, 512]

# üöÄ DYNAMIC CONTEXT LENGTH EXAMPLES (Repository-validated patterns)
def dynamic_context_forecast(guardian, context_data, prediction_length=12):
    """Demonstrate TiRex dynamic context length capability"""
    base_context = prepare_tirex_input(context_data, context_length=512)  # Prepare long context

    # Runtime context length adjustment via max_context parameter
    short_forecast = guardian.safe_forecast(
        context=base_context,
        prediction_length=prediction_length,
        max_context=144  # ‚Üê DYNAMIC: Use only last 144 timesteps at runtime
    )

    medium_forecast = guardian.safe_forecast(
        context=base_context,
        prediction_length=prediction_length,
        max_context=288  # ‚Üê DYNAMIC: Use only last 288 timesteps at runtime
    )

    return short_forecast, medium_forecast

# üîÑ VARIABLE LENGTH BATCH PROCESSING (Auto-padded by TiRex)
variable_contexts = [
    prepare_tirex_input(market_data, context_length=144)[0],  # 144 timesteps
    prepare_tirex_input(market_data, context_length=288)[0],  # 288 timesteps
    prepare_tirex_input(market_data, context_length=512)[0]   # 512 timesteps
]
# TiRex handles variable lengths automatically with left-padding!
```

---

#### Critical Optimization Questions (Empirically-Informed)

**üî¨ PRIMARY: Empirically Validated Approaches**

1. **Series Selection**: Raw close prices (repository-proven) vs typical price vs volume-weighted - A/B test for specific asset
2. **Context Length**: 128 vs 288 vs 512 timesteps - empirical testing required for optimal window
3. **Data Quality**: Use NaN placeholders for gaps (TiRex native handling) vs interpolation strategies

**üìä SECONDARY: Architecture Configuration**

4. **Native TiRex Parameters**: Use model defaults (patch_size from config) - no repository evidence of manual tuning benefits
5. **Missing Value Strategy**: Leverage TiRex native NaN handling vs preprocessing imputation
6. **Market Session Effects**: Include/exclude weekend/holiday periods in context window

**‚ö†Ô∏è AVOID: No Repository Evidence**

~~7. Log returns or financial transformations~~ (zero repository evidence)
~~8. Manual normalization or stationarity transforms~~ (TiRex StandardScaler optimal)
~~9. Complex preprocessing pipelines~~ (contradicts empirical simplicity success)

---

#### Implementation Roadmap ‚Äî Univariate Optimization

##### Phase 1: Input Series Optimization (HIGH Impact)

- **Target**: Test different univariate input series for TiRex
- **Focus**: Raw close, log returns, typical price, volume-weighted price
- **Expected Gain**: 10-20% improvement from optimal series selection
- **Timeline**: Immediate A/B testing

##### Phase 2: Preprocessing Enhancement (MEDIUM Impact)

- **Target**: Optimize normalization and data cleaning
- **Focus**: Regime-aware scaling, outlier handling, gap filling
- **Expected Gain**: Additional 5-15% improvement from data quality
- **Timeline**: After Phase 1 validation

##### Phase 3: Multi-Model Strategy (FUTURE)

- **Target**: Ensemble multiple TiRex models
- **Focus**: Different assets, timeframes, preprocessing methods
- **Expected Gain**: Portfolio-level improvement through diversification
- **Timeline**: Long-term strategic enhancement

---

#### Risk Assessment & Mitigation

##### Technical Risks

- **Computational Complexity**: 8-feature processing may increase inference latency
  - _Mitigation_: Benchmark performance vs accuracy trade-offs
- **Data Pipeline Complexity**: Multi-feature preprocessing increases failure points
  - _Mitigation_: Robust error handling and fallback mechanisms

##### Financial Risks

- **Over-fitting**: Rich feature set may not generalize across market conditions
  - _Mitigation_: Systematic backtesting across multiple market regimes
- **Regime Dependency**: Optimization may be specific to certain market structures
  - _Mitigation_: Multi-market validation and adaptive parameters

##### Implementation Risks

- **Integration Complexity**: Guardian system integration with enhanced features
  - _Mitigation_: Phased rollout with comprehensive testing
- **Maintenance Burden**: 8-feature system requires more sophisticated monitoring
  - _Mitigation_: Automated validation and performance tracking

---

#### Success Metrics & Validation

##### Performance Benchmarks

- **Quantitative Improvement**: 2-4x better forecast accuracy vs current baseline
- **Architecture Utilization**: 100% of TiRex `input_patch_size * 2` capacity
- **Inference Speed**: Maintain <100ms inference time for real-time trading
- **Memory Efficiency**: Manage GPU memory usage within production constraints

##### Financial Validation

- **Sharpe Ratio Improvement**: 50%+ improvement in risk-adjusted returns
- **Drawdown Reduction**: 25%+ reduction in maximum drawdown periods
- **Alpha Generation**: Statistically significant alpha over baseline strategies
- **Market Adaptability**: Consistent performance across different volatility regimes

---

#### Integration with Guardian System ‚Äî ‚úÖ Production Validated

##### üèÜ EMPIRICALLY VALIDATED Guardian Usage (RTX 4090 Testing Results)

```python
from sage_forge.guardian import TiRexGuardian

# ‚úÖ PRODUCTION-READY: Guardian system fully debugged and validated
guardian = TiRexGuardian(
    threat_detection_level="medium",
    data_pipeline_protection="strict",     # 100% vulnerability coverage
    fallback_strategy="graceful"
)

# ‚úÖ EMPIRICALLY OPTIMIZED: Context preparation with validated lengths
close_series = prepare_univariate_context(
    raw_context=market_data,
    series_type="close",           # Single series selection  
    preprocessing="minimal",       # TiRex StandardScaler optimal
    context_length=384            # ‚úÖ EMPIRICALLY FASTEST: 9.4ms per prediction
)

# ‚úÖ PRODUCTION VALIDATED: Guardian with <1ms overhead
predictions_quantiles, predictions_mean = guardian.safe_forecast(
    context=close_series,         # [1, 384] - optimal context length
    prediction_length=12,         # 1 hour ahead
    user_id="production_forecasting"
)

# Guardian System Status: ‚úÖ PRODUCTION READY
# - Overhead: <1ms additional processing time
# - Success Rate: 100% (all test predictions completed)
# - Security Events: 0 blocks during normal operation  
# - Protection: 6 vulnerability categories fully covered
```

**üõ°Ô∏è GUARDIAN VALIDATION RESULTS** (Comprehensive Testing):

- **Performance Impact**: Negligible (<1ms overhead)
- **Protection Coverage**: 100% across all 6 vulnerability categories
- **Success Rate**: 100% - all predictions completed successfully
- **Memory Overhead**: Minimal (included in context length measurements)
- **Production Status**: ‚úÖ **READY FOR IMMEDIATE DEPLOYMENT**

---

#### Conclusion ‚Äî TiRex Univariate Reality

The TOKENIZED layer processing reveals **TiRex's univariate architecture** - designed for single time series forecasting excellence.

**üìã EMPIRICALLY VALIDATED**: This conclusion is backed by comprehensive testing and source code analysis:

- **üî¨ Source Code Proof**: `assert data.ndim == 2` hardcoded in `PatchedUniTokenizer`
- **üß™ Empirical Tests**: All multi-dimensional inputs rejected with AssertionError
- **‚úÖ Test Results**: [Complete Validation Report](../../implementation/tirex/empirical-validation/TIREX_EMPIRICAL_FINDINGS_COMPREHENSIVE.md)

**Key Understanding**: TiRex is NOT a multi-feature model but excels at univariate forecasting:

- `PatchedUniTokenizer` optimized for single time series processing
- `StandardScaler` designed for univariate normalization
- sLSTM architecture tuned for temporal patterns in single variable
- Patch processing applies to time dimension, not feature dimension

**üèÜ PRODUCTION-VALIDATED OPTIMIZATION STRATEGY** (RTX 4090 Empirical Testing):

- **384 timesteps for speed-critical applications** (9.4ms - empirically fastest)
- **288 timesteps for standard production** (9.5ms - proven reliable baseline)
- **512 timesteps for quality-focused systems** (9.7ms - minimal speed penalty)
- **2048+ timesteps for research applications** (9.3-10.1ms - multi-day analysis capability)

**üîó COMPREHENSIVE VALIDATION EVIDENCE**:

- [Guardian System Debug Report](../../../tests/performance/context_length_empirical_suite/ULTRATHINKING_SUCCESS_SUMMARY.md)
- [Intermediate Context Validation](../../../tests/performance/context_length_empirical_suite/results/intermediate_benchmark_1754933887.csv)
- [Large Context Validation](../../../tests/performance/context_length_empirical_suite/results/large_context_benchmark_1754934204.csv)
- [Guardian Production Testing](../../../tests/performance/context_length_empirical_suite/corrected_guardian_benchmark.py)

**üöÄ PRODUCTION-READY OPTIMIZATION ROADMAP**:

**‚úÖ Phase 1: Immediate Production Deployment**:

1. **Context Length Selection**: Use empirically-validated configurations (384/288/512 timesteps)
2. **Guardian System Integration**: Deploy production-ready security with <1ms overhead
3. **Memory Optimization**: Leverage flat scaling (310MB for all large contexts)

**‚úÖ Phase 2: Performance Monitoring**: 
4. **Real-time Metrics**: Monitor inference times against empirical baselines (9.4-10.1ms)
5. **Context Length A/B Testing**: Validate optimal contexts for specific trading strategies  
6. **Guardian Performance Tracking**: Monitor protection effectiveness and overhead

**‚úÖ Phase 3: Advanced Capabilities**: 
7. **Large Context Research**: Utilize 2K-16K timesteps for long-range dependency analysis
8. **Dynamic Context Switching**: Runtime optimization based on market conditions
9. **Multi-GPU Scaling**: Leverage flat memory profile for distributed inference

---

[‚Üê Back to Index](../strategy-io-contract.md#layer-navigation-tirex-native) | [Next: PREDICTIONS Layer ‚Üí](./predictions-layer.md)
